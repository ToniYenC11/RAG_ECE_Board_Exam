{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55beb70c",
   "metadata": {},
   "source": [
    "# LangChain Integration With Hugging Face\n",
    "\n",
    "**Hugging Face** has provided the means to use language models with zero cost for API use. **Lang Chain** is a library that makes it possible to create LLM applications by integrating it with you software. \n",
    "\n",
    "This notebook makes it possible to learn how to integrate Hugging Face models with LangChain to create your own specialized LLMs with minimal dependency on the commercial paywall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16367f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "llm.invoke(\"What is DOST?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987ae0d",
   "metadata": {},
   "source": [
    "## Using Prompt Templates\n",
    "\n",
    "Prompt templates allow the system to format the output based on the instruction inside the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* Few shot prompt template\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"What is the ASTHRD Scholarship?\",\n",
    "    \"answer\": \"\"\"The Accelerated Science and Technology Human Resource Development Program (ASTHRDP) scholarship\n",
    "    of the Department of Science and Technology (DOST) for individuals intending to pursue Masterâ€™s and Doctoral programs in priority S&T areas\n",
    "    in the country. Scholars are encouraged to choose topics for their thesis/dissertation along the following areas:\n",
    "    \n",
    "    1. Climate Change and Disaster Preparedness\n",
    "    2. Materials Science and Nanotechnology\n",
    "    3. Natural Products and Drug Development\n",
    "    \"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the scope of the examination for the DOST scholarship\",\n",
    "    \"answer\": \"\"\"Assuming you are referring to the undergraduate scholraship and the Junior Level Science Scholarship (JLSS), the subjects involved are the following:\n",
    "    1. Logical Reasoning - Situational questions, logical thinking, critical analysis, problem-solving\n",
    "    2. English - Grammar, reading comprehension, parts of a sentence, figures of speech\n",
    "    3. Science - Biology, Physics, Chemistry, Earth Scince, Advanced Sciences such as Environmental Science, Electronics\n",
    "    4. Mathematics - Algebra, Basic Calculus, Precalculus, Statistics and Probability, Numerical Methods\n",
    "    5. Mechanical-Technical - More on situations involving engineering principles and the emerging technologies today\n",
    "    6. Self-Inventory - Soft skills such as collaboration, communication, adaptiveness, among other skills\n",
    "    \"\"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "# Example prompt of Question and Answer\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "llm_chain = prompt_template | llm #* pipe the prompt_template into the llm\n",
    "#print(llm_chain.invoke({\"input\":\"What is the ERDT Scholarship of DOST?\"})) #Currently in debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d28294",
   "metadata": {},
   "source": [
    "## Building Chains\n",
    "\n",
    "Chains is chaining activities from a prompt to a model to a parser, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that takes an input activity\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "# Create a prompt template that places a time constraint on the output\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables = ['learning_plan'],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Invoke the learning_prompt with an activity\n",
    "print(learning_prompt.invoke({\"activity\": \"play golf\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* Another chain using LCEL (LangChain Expression Language)\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=[\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a concise plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Complete the sequential chain with LCEL\n",
    "seq_chain = ({\"learning_plan\": learning_prompt | llm | StrOutputParser()}\n",
    "    | time_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "# Call the chain\n",
    "print(seq_chain.invoke({\"activity\": \"Destroying Russia\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36875b",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "Agents is the ise of LLMS to take action. It uses **tools** whic hare functions to extract a data the agent needs in order to respond.\n",
    "\n",
    "The most fundamental agent is a **ReAct** agent which combines a _Reason_ with an _Action_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "# Define the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent.invoke({\"messages\": [(\"human\", \"How many people live in New York City?\")]})\n",
    "print(response['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e9a21",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "LangChain agents uses tools to request data from, and in return use the data to make a response to the user. Below is a sample invokation for an agent using the tool \"Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405600ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve customer info by-name\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    # Filter customers for the customer's name\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Call the function on Peak Performance Co. (this is now known as a tool)\n",
    "print(retrieve_customer_info(\"Peak Performance Co.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46318e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool # Converts the function into a tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "\n",
    "# Create a ReAct agent\n",
    "agent = create_react_agent(llm,[retrieve_customer_info])\n",
    "\n",
    "# Invoke the agent on the input\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"Create a summary of our customer: Peak Performance Co.\")]})\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2e85b",
   "metadata": {},
   "source": [
    "# Retreival Augmented Generation\n",
    "\n",
    "The steps are as follows:\n",
    "1. Document Loaders - Docling most versatile as it can handle any document.\n",
    "2. Data splitting - **Chunk overlap** is necessary to maintain context\n",
    "    - `\\n\\n` - Paragraph split\n",
    "    - `\\n` - Sentence split\n",
    "3. Storage + Retrieval (Vector databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9996b9e",
   "metadata": {},
   "source": [
    "### Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "#* Document Loaders\n",
    "\n",
    "# Create a document loader for unstructured HTML\n",
    "loader = UnstructuredHTMLLoader('white_house_executive_order_nov_2023.html')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "\n",
    "# Print the first document\n",
    "print(data[0])\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db98a49",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "#* Text splitter\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter('\\n',chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the string and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#* Recursive splitter allows you to split the string based on a sequence of escape characters\n",
    "# Here, it first splits by \\n, then by whitespace, then by a null string.\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(['\\n',\" \",\"\"],chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f86bc",
   "metadata": {},
   "source": [
    "### Storing and Retrieval in Vector Databases\n",
    "\n",
    "See [this link](https://python.langchain.com/docs/integrations/vectorstores/chroma/) for an example with Chroma. Other vector stores are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32382cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('rag_vs_fine_tuning.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "# Split the document using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=50)\n",
    "docs = splitter.split_documents(data)\n",
    "\n",
    "# Embed the documents in a persistent Chroma vector database\n",
    "embedding_function = OpenAIEmbeddings(api_key='<OPENAI_API_TOKEN>', model='text-embedding-3-small')\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "# Configure the vector store as a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, create Prompt Template\n",
    "# Add placeholders to the message string\n",
    "message = \"\"\"\n",
    "Answer the following question using the context provided:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message string\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings(api_key='<OPENAI_API_TOKEN>', model='text-embedding-3-small'),\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Create a chain to link retriever, prompt_template, and llm\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm)\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(\"Which popular LLMs were considered in the paper?\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2_GEAS-DmZNTpg5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
